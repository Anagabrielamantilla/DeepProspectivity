# -*- coding: utf-8 -*-
"""05_FDE_FT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uUVh0DAR0lDUR69ehclp6XfKR-Y9Qrti
"""

# Elaborado por:
# Ana Mantilla : anagmd2019@gmail.com
# Paul Goyes : goyes.yesid@gmail.com

#Importar librerías
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from osgeo import gdal
from tensorflow import keras
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sklearn import metrics
from tensorflow.keras.models import Model

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Se importan los datos de entrenamiento en formato .csv con la librería pandas. A esta variable la llamaremos df

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/TRAINING/02_Train_PCA.csv', sep=';')
df

#Ahora vamos a extraer los valores de los componentes principales PC1-PC9 y de las etiquetas de ocurrencia mineral (1) y no-ocurrencia mineral (0)

y_names=['DEP']
y=df[y_names].values

x_names=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9']
X=df[x_names].values

# Crear el modelo de red autoencoder. Esta red tendrá una configuración 9-100-8-6-4-2-4-6-8-100-9


model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(9,))) # CAPA INPUT
model.add(tf.keras.layers.Dense(100, activation="relu")) # 1 CAPA OCULTA, TIENE 100 NEURONAS
model.add(tf.keras.layers.Dense(8, activation="relu")) # 2 CAPA OCULTA, TIENE 8 NEURONAS
model.add(tf.keras.layers.Dense(6, activation="relu")) # 3 CAPA OCULTA, TIENE 6 NEURONAS
model.add(tf.keras.layers.Dense(4, activation="relu")) # 4 CAPA OCULTA, TIENE 4 NEURONAS
model.add(tf.keras.layers.Dense(2, activation="relu")) # 5 CAPA OCULTA, TIENE 2 NEURONAS
model.add(tf.keras.layers.Dense(4, activation=tf.keras.layers.LeakyReLU())) # 6 CAPA OCULTA, TIENE 4 NEURONAS
model.add(tf.keras.layers.Dense(6, activation=tf.keras.layers.LeakyReLU())) # 7 CAPA OCULTA, TIENE 6 NEURONAS
model.add(tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU())) # 8 CAPA OCULTA, TIENE 8 NEURONAS
model.add(tf.keras.layers.Dense(100, activation=tf.keras.layers.LeakyReLU())) # 8 CAPA OCULTA, TIENE 100 NEURONAS
model.add(tf.keras.layers.Dense(9, activation=tf.keras.layers.LeakyReLU())) # 9 CAPA OCULTA, TIENE 9 NEURONAS

model.summary()

# Imprimimos un diagrama grado del modelo

tf.keras.utils.plot_model(model, show_shapes=True)

# Indicamos el optimizador y las métricas. En este caso usaremos el optimizador Adam, con un learning rate de 1e-3 y la función de pérdida MSE. El modelo se compila con la métrica 'accuracy' y procede a ser entrenado

optimizer    = tf.keras.optimizers.Adam(learning_rate=1e-3)
loss_fn  = tf.keras.losses.MeanSquaredError()

model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

#Entrenar el modelo de red autoencoder. En este caso usamos un batch size de 10000 debido a que la entrada
#de la red corresponde al número de pixeles en las capas de entrada. Usamos 350 épocas y 20% de los datos para validación

history= model.fit(
    x=XX,
    y=XX,
    batch_size=10000,
    epochs=350,
    validation_split=0.2
    )

#Graficar las métricas del modelo de red autoencoder. Se calculará la precisión y valor de la función de pérdida durante el entrenamiento

score= model.evaluate(XX,XX)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Graficar las métricas

fig, ax1 = plt.subplots(1,1,figsize=(10,5))
ax1.tick_params(axis='y', labelcolor='blue')
ax1.plot(history.history['loss'],'b',label='LOSS')
ax1.set_ylabel('Función de pérdida', color='blue')
ax1.set_xlabel('Número de épocas', color='black')
ax2 = ax1.twinx()
ax2.set_ylabel('Precisión', color='red')
ax2.plot(history.history['accuracy'],'red')
ax2.tick_params(axis='y', labelcolor='red')

#Guardar el modelo de red autoencoder. Para esto usamos la función model.save() de keras. Se llama modelo_encoder y se guarda en formato .h5


model.save("/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-PRE-ENTRENADOS/modelo_encoder.h5")

#Cargar el modelo entrenado autoencoder

model = keras.models.load_model("/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-PRE-ENTRENADOS/modelo_encoder.h5")
model.summary()

#Creamos un nuevo modelo que solo incluya las capas del encoder

encoder = Model(inputs=model.input, outputs=model.layers[4].output)

#Calculamos los pesos de cada clase

pos = len(y[y==1])
neg = len(y[y==0])
total= pos+neg
print('pos: ', pos)
print('neg:' , neg)

weight_for_0 = (1 / neg) * (total/2 )
weight_for_1 = (1 / pos) * (total/2)

class_weight = {0: weight_for_0, 1: weight_for_1}

weight_for_1

weight_for_0

#Creamos el modelo clasificador con una función de inicialización para el bias

initial_bias = np.log(pos/neg)
print(initial_bias)
output_bias = tf.keras.initializers.Constant(initial_bias)

latent = tf.keras.layers.Dense(1, activation="sigmoid", bias_initializer = output_bias, name='new')(encoder.layers[-1].output)
model_classifier = Model(inputs=encoder.input, outputs=latent)

# Imprimir un diagrama de grafo del modelo

tf.keras.utils.plot_model(model_classifier, show_shapes=True)

# Indicar el optimizador usado para el backpropagation y las métricas de evaluación

optimizer    = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss_fn      =  tf.keras.losses.BinaryCrossentropy()

# Compilar el modelo con los hiperparámetros que se definieron anteriormente

model_classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

# Transferencia de aprendizaje por fine-tuning. Entrenamos el modelo clasificador con 2000 épocas y
# ajustamos el peso para cada clase. Esto ayudará a lidiar con el imbalance de datos


history= model_classifier.fit(
    x=X,
    y=y,
    batch_size=82,
    epochs=2000, class_weight = class_weight
    )

#Guardamos las métricas en formato .npz

np.savez('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/METRICAS-MODELOS/FDE_FT.npz', loss=history.history['loss'], acc=history.history['accuracy'])

# Graficar las métricas

fig, ax1 = plt.subplots(1,1,figsize=(10,5))
ax1.tick_params(axis='y', labelcolor='blue')
ax1.plot(metrica_FDE_FT['loss'],'b',label='LOSS')
ax1.set_ylabel('Función de pérdida', color='blue')
ax1.set_xlabel('Número de épocas', color='black')
ax2 = ax1.twinx()
ax2.set_ylabel('Precisión', color='red')
ax2.plot(metrica_FDE_FT['acc'],'red')
ax2.tick_params(axis='y', labelcolor='red')

#Guardar el modelo clasificador. Se guardan todos los hiperparámetros, configuración y pesos de la red

model_classifier.save('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-FINALES/autoencoder_fine.h5')

# Graficar las métricas del modelo clasificador. Graficamos la precisión y la función de pérdida


# Calcular la precisión y valor de la función de pérdida del entrenamiento

score= model_classifier.evaluate(X,y)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Graficar las métricas

fig, ax1 = plt.subplots(1,1,figsize=(10,5))
ax1.tick_params(axis='y', labelcolor='blue')
ax1.plot(history.history['loss'],'b',label='LOSS')
ax1.set_ylabel('Función de pérdida', color='blue')
ax1.set_xlabel('Número de épocas', color='black')
ax2 = ax1.twinx()
ax2.set_ylabel('Precisión', color='red')
ax2.plot(history.history['accuracy'],'red')
ax2.tick_params(axis='y', labelcolor='red')

# Realizamos una predicción de los datos para calcular la matriz de confusión


pred_val = model_classifier.predict(X)

# # Matriz de confusión


matriz3 = confusion_matrix(y, np.around(pred_val))
plot_confusion_matrix(conf_mat = matriz3, figsize = (5,5))

ax = plt.subplot()
sns.set(font_scale=1) # Adjust to fit
sns.heatmap(matriz3, annot=True, ax=ax, cmap="Blues", fmt="g");

# Labels, title and ticks
label_font = {'size':'18'}  # Adjust to fit
ax.set_xlabel('Predicted labels');
ax.set_ylabel('Observed labels',fontdict=label_font);

title_font = {'size':'18'}  # Adjust to fit
ax.set_title('Confusion Matrix', fontdict=title_font);

ax.tick_params(axis='both', which='major', labelsize=18)  # Adjust to fit
ax.xaxis.set_ticklabels(['False', 'True']);
ax.yaxis.set_ticklabels(['False', 'True']);

# Calculamos el AUC mediante la gráfica del ROC


fpr_, tpr_, thresholds_ = roc_curve(y, pred_val)

plt.plot(fpr_, tpr_, marker='o')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.title('ROC CURVE')
plt.grid(True)

# # Cálculo del AUC


print(f'El AUC para el modelo es de: {metrics.auc(fpr_, tpr_)}')

# Cargar el ráster virtual para hacer la predicción. Cargamos el ráster virtual y extraemos sus características, tales como número de filas y columnas

raster  = gdal.Open('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/VIRTUAL_RASTER/02_PCA.tif')

# Extraer las características del ráster

driver = raster.GetDriver()
col   = raster.RasterXSize #número de columnas
rows  = raster.RasterYSize #número de filas
nelem = col*rows #número de píxeles

print('numero de filas: ', rows)
print('numero de columnas: ', col)

# Acondicionamiento del ráster virtual. Guardamos y eliminamos las posiciones NonValue.
# Solo se hace una vez para cualquier banda ya que las posiciones de Nonvalue deben ser las mismas para todas las bandas


Nonvalue = raster.GetRasterBand(1).GetNoDataValue()

print(Nonvalue)

#Extraer los valores de cada banda

v1val = raster.GetRasterBand(1).ReadAsArray().flatten()
v2val = raster.GetRasterBand(2).ReadAsArray().flatten()
v3val = raster.GetRasterBand(3).ReadAsArray().flatten()
v4val = raster.GetRasterBand(4).ReadAsArray().flatten()
v5val = raster.GetRasterBand(5).ReadAsArray().flatten()
v6val = raster.GetRasterBand(6).ReadAsArray().flatten()
v7val = raster.GetRasterBand(7).ReadAsArray().flatten()
v8val = raster.GetRasterBand(8).ReadAsArray().flatten()
v9val = raster.GetRasterBand(9).ReadAsArray().flatten()

# Agrupar los valores tal que se forme una matriz de N X 9
# donde N es el número de muestras

DATA = np.stack((v1val,v2val,v3val,v4val,v5val,v6val,v7val,v8val,v9val),axis=1)


# Organizar los datos y eliminar las posiciones de NonData

NanValues = np.where(v1val == Nonvalue)[0]
cP        = np.arange(0,nelem)
cPP       = np.delete(cP, NanValues, axis=0)

XX = np.delete(DATA, NanValues, axis=0)

# Predicción de la probabilidad en toda la zona de estudio. Obtenemos los valores de probabilidad en las zonas donde no hay puntos conocidos

y_raster = model_classifier.predict(XX)

# Exportar los valores de probabilidad calculados. Usamos una variable nueva llena de zeros,
# en la cual insertaremos los NonValues y los valores de la predicción. El tamaño de esta variable será el mismo que el raster

Rasterdataarray = np.zeros((rows,col)).flatten()

# cPP contiene las posiciones donde van las predicciones
for i in range(cPP.shape[0]):
    Rasterdataarray[cPP[i]]=y_raster[i]

# NanValues contiene las posiciones donde van los NOnValues
for i in range(NanValues.shape[0]):
    Rasterdataarray[NanValues[i]]=Nonvalue


# guardar la predicción en un raster con las mismas caracteristicas del raster input

Rasterout = driver.Create('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MAPA_PROBABILIDAD/02_Autoencoder_Fine_Tuning.tif', col, rows, 1, gdal.GDT_Float32) #esta ruta se modifica según donde desee que se guarde el mapa de probabilidad en su Google Drive
# Write metadata
Rasterout.SetGeoTransform(raster.GetGeoTransform())
Rasterout.SetProjection(raster.GetProjection())

Rasterout.GetRasterBand(1).WriteArray(Rasterdataarray.reshape(rows,col))
Rasterout.GetRasterBand(1).SetNoDataValue(Nonvalue)
Rasterout = None
del Rasterout

# Visualizar los valores de probabilidad calculados

mask = Rasterdataarray!=-9999
v2 = Rasterdataarray[np.where(mask)]
v2 = v2.reshape(-1,961)
plt.figure(figsize=(10,10))
plt.imshow(v2,cmap='jet')
plt.colorbar(fraction=0.04)

"""# **Créditos**
---

* **Autores:**
  * [Ana Gabriela Mantilla, Geóloga](https://www.linkedin.com/in/ana-gabriela-mantilla-24377a21a)
  * [Paul Goyes Peñafiel, PhD (c)](https://www.linkedin.com/in/paul-goyes-0212b810/)
"""