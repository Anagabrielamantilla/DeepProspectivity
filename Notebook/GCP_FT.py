# -*- coding: utf-8 -*-
"""09_GCP_FT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13cD_-XlhDeAKHK-uvRlBed7dZYCGUQ3Z
"""

# Elaborado por:
# Ana Mantilla : anagmd2019@gmail.com
# Paul Goyes : goyes.yesid@gmail.com

# Importar las librerías

import pandas as pd #usada para cargar los datos delimitados por comas (.csv)
import numpy as np #usada para extraer los valores de píxel del ráster en una matriz
import matplotlib.pyplot as plt #usada para gráficar
import tensorflow as tf #usada para crear el modelo de redes neuronales artificiales
from tensorflow import keras
from tensorflow.keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

#Importar los datos de la tarea pretexto. En este caso corresponde a las 5 categorías de roca

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/TRAINING/03_Train_Pretext.csv', sep =';')

df.shape

# Visualizar la tabla de datos

df

#Extraer las variables de entrada y de salida de la tabla

y_names=['ETIQUETA'] # Valores de salida-etiquetas de depósito (1) y no depósito (0)
y=df[y_names].values

x_names=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9']
X=df[x_names].values

#One hot encoding

from tensorflow.keras.utils import to_categorical
trainY = to_categorical(y)

trainY

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, trainY, test_size=0.2, shuffle=True)

# Crear el modelo de la tarea pretexto


model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(9,))) # CAPA INPUT
model.add(tf.keras.layers.Dense(100, activation="relu")) # 1 CAPA OCULTA, TIENE 100 NEURONAS
model.add(tf.keras.layers.Dense(8, activation="relu")) # 2 CAPA OCULTA, TIENE 8 NEURONAS
model.add(tf.keras.layers.Dense(6, activation="relu")) # 3 CAPA OCULTA, TIENE 6 NEURONAS
model.add(tf.keras.layers.Dense(4, activation="relu")) # 4 CAPA OCULTA, TIENE 4 NEURONAS
model.add(tf.keras.layers.Dense(2, activation="relu")) # 5 CAPA OCULTA, TIENE 2 NEURONAS
model.add(tf.keras.layers.Dense(5, activation='softmax')) # 9 CAPA OCULTA PRETEXT, TIENE 5 NEURONAS

# Imprimir un resumen del modelo

model.summary()

# Imprimir un diagrama de grafo del modelo

tf.keras.utils.plot_model(model, show_shapes=True)

# Indicar el optimizador usado para el backpropagation y las métricas de evaluación

optimizer    = tf.keras.optimizers.Adam(learning_rate=1e-3) #antes learning rate 1e-3
loss_fn  = tf.keras.losses.CategoricalCrossentropy() # tambien se puede indicar la función de perdida como las predeterminadas en keras

# Compilar el modelo con los hiperparámetros que se definieron anteriormente

model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

# Entrenar el modelo de la tarea pretexto

history= model.fit(
    x=X_train,
    y=y_train,
    epochs=1000,
    validation_data=(X_test, y_test)
    )

# En la variable "history" quedan guardadas todas las métricas y funciones de perdida evaluadas en cada época.
# Podemos hacer una gráfica para cada una de ellas, primero vemos lo que podemos gráficar:


print(history.history.keys())

prediccion = model.predict(X)

### Calcular la precisión y valor de la función de pérdida del entrenamiento del modelo de la tarea pretexto

score= model.evaluate(X_test,y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1]) #aqui medir absolute error

# # Graficar las métricas del modelo de la tarea pretexto

fig, ax1 = plt.subplots(1,1,figsize=(10,5))
ax1.tick_params(axis='y', labelcolor='blue')
ax1.plot(history.history['loss'],'b',label='LOSS')
ax1.plot(history.history['val_loss'],'--b',label='LOSS VAL')
ax1.set_ylabel('Función de pérdida', color='blue')
ax1.set_xlabel('Número de épocas', color='black')
ax2 = ax1.twinx()
ax2.set_ylabel('Precisión', color='red')
ax2.plot(history.history['accuracy'],'red')
ax2.plot(history.history['val_accuracy'],'--r')
ax2.tick_params(axis='y', labelcolor='red')

#convertir predicciones a one hot

class_pre = np.argmax(prediccion, axis =1)

plt.scatter(df['E'], df['N'], c = class_pre)
plt.colorbar()

# Guardar el modelo de la tarea pretexto

model.save("/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-PRE-ENTRENADOS/modelo_pretext.h5")

# Cargar el modelo preentrenado de la tarea pretexto
model = keras.models.load_model("/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-PRE-ENTRENADOS/modelo_pretext.h5")

# Imprimir resumen de la arquitectura del modelo de la tarea pretexto

model.summary()

# Cargar el conjunto de datos de entrenamiento con las variables de entrada - componentes principales

df_yukon=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/TRAINING/02_Train_PCA.csv', sep=';')

# Extraer variables de entrada y de salida

y_names_yukon=['DEP'] # Valores de salida-etiquetas de depósito (1) y no depósito (0)
y_yukon=df_yukon[y_names_yukon].values

x_names_yukon=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9']
X_yukon=df_yukon[x_names_yukon].values

# Crea un nuevo modelo que incluya solo las capas específicas

clasifier = Model(inputs=model.input, outputs=model.layers[-2].output)

# Calcular peso de cada clase y el bias inicial

pos = len([y_yukon==1])
neg = len(y_yukon[y_yukon==0])
total= pos+neg
print('pos: ', pos)
print('neg:' , neg)

weight_for_0 = (1 / neg) * (total/2 )
weight_for_1 = (1 / pos) * (total/2)

class_weight = {0: weight_for_0, 1: weight_for_1}

initial_bias = np.log(pos/neg)
print(initial_bias)
output_bias = tf.keras.initializers.Constant(initial_bias)

# Imprimir resumen de la arquitectura del clasificador

clasifier.summary()

# Agregar la capa de salida con activación sigmode y función de activación del bias

deposito = tf.keras.layers.Dense(1, activation="sigmoid", bias_initializer = output_bias, name="Deposito")(clasifier.layers[-1].output)
model_classifier = Model(inputs=clasifier.input, outputs=deposito)

# Imprimir resumen de la arquitectura del clasificador nuevo

model_classifier.summary()

# Imprimir un diagrama de grafo del modelo

tf.keras.utils.plot_model(model_classifier, show_shapes=True)

# Indicar el optimizador usado para el backpropagation y las métricas de evaluación

optimizer    = tf.keras.optimizers.Adam(learning_rate=1e-3)
loss_fn      =  tf.keras.losses.BinaryCrossentropy() # tambien se puede indicar la función de perdida como las predeterminadas en keras

# Compilar el modelo con los hiperparámetros que se definieron anteriormente

model_classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

# Transferencia de aprendizaje. Entrenamiento del modelo clasificador que tiene lo pesos pre entrenados

history= model_classifier.fit(
    x=X_yukon,
    y=y_yukon,
    batch_size=82,
    epochs=2000,
    class_weight = class_weight
    )

# En la variable "history" quedan guardadas todas las métricas y funciones de perdida evaluadas en cada época.
# Podemos hacer una gráfica para cada una de ellas, primero vemos lo que podemos gráficar:


print(history.history.keys())

# Guardar las métricas del modelo en formato .npz

np.savez('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/METRICAS-MODELOS/GPC_FT.npz', loss=history.history['loss'], acc=history.history['accuracy'])

# Calcular la precisión y valor de la función de pérdida del entrenamiento

score= model_classifier.evaluate(X_yukon,y_yukon)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Graficar las métricas

fig, ax1 = plt.subplots(1,1,figsize=(10,5))
ax1.tick_params(axis='y', labelcolor='blue')
ax1.plot(history.history['loss'],'b',label='LOSS')
ax1.set_ylabel('Función de pérdida', color='blue')
ax1.set_xlabel('Número de épocas', color='black')
ax2 = ax1.twinx()
ax2.set_ylabel('Precisión', color='red')
ax2.plot(history.history['accuracy'],'red')
ax2.tick_params(axis='y', labelcolor='red')

# Predecir los datos

pred_val = model_classifier.predict(X_yukon)

# Matriz de confusión

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

matriz3 = confusion_matrix(y_yukon, np.around(pred_val))
plot_confusion_matrix(conf_mat = matriz3, figsize = (5,5))

ax = plt.subplot()
sns.set(font_scale=1) # Adjust to fit
sns.heatmap(matriz3, annot=True, ax=ax, cmap="Blues", fmt="g");

# Labels, title and ticks
label_font = {'size':'18'}  # Adjust to fit
ax.set_xlabel('Predicted labels');
ax.set_ylabel('True labels',fontdict=label_font);

title_font = {'size':'18'}  # Adjust to fit
ax.set_title('Confusion Matrix', fontdict=title_font);

ax.tick_params(axis='both', which='major', labelsize=18)  # Adjust to fit
ax.xaxis.set_ticklabels(['False', 'True']);
ax.yaxis.set_ticklabels(['False', 'True']);

# Cálculo del ROC

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc

fpr_, tpr_, thresholds_ = roc_curve(y_yukon, pred_val)

plt.plot(fpr_, tpr_, marker='o')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.title('ROC CURVE')
plt.grid(True)

# Cálculo del AUC

from sklearn import metrics

print(f'El AUC para el modelo es de: {metrics.auc(fpr_, tpr_)}')

# Guardar el modelo clasificador entrenado

model_classifier.save('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MODELOS-FINALES/pretext_fine.h5')

# Cargar el virtual ráster que se descarga desde el link del repositorio

from osgeo import gdal

raster  = gdal.Open('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/VIRTUAL_RASTER/02_PCA.tif')  # Esta ruta cambia según dónde se guardó el archivo del virtual ráster

# Extraer las características del ráster

driver = raster.GetDriver()
col   = raster.RasterXSize #número de columnas
rows  = raster.RasterYSize #número de filas
nelem = col*rows #número de píxeles

print('numero de filas: ', rows)
print('numero de columnas: ', col)

# Guardar las posiciones NonData del ráster. Solo se hace una vez para cualquier banda
# ya que las posiciones de Nonvalue deben ser las mismas para todas las bandas


Nonvalue = raster.GetRasterBand(1).GetNoDataValue()

print(Nonvalue)

#Extraer los valores de cada banda

v1val = raster.GetRasterBand(1).ReadAsArray().flatten()
v2val = raster.GetRasterBand(2).ReadAsArray().flatten()
v3val = raster.GetRasterBand(3).ReadAsArray().flatten()
v4val = raster.GetRasterBand(4).ReadAsArray().flatten()
v5val = raster.GetRasterBand(5).ReadAsArray().flatten()
v6val = raster.GetRasterBand(6).ReadAsArray().flatten()
v7val = raster.GetRasterBand(7).ReadAsArray().flatten()
v8val = raster.GetRasterBand(8).ReadAsArray().flatten()
v9val = raster.GetRasterBand(9).ReadAsArray().flatten()

# Agrupar los valores tal que se forme una matriz de N X 9
# donde N es el número de muestras

DATA = np.stack((v1val,v2val,v3val,v4val,v5val,v6val,v7val,v8val,v9val),axis=1)


# Organizar los datos y eliminar las posiciones de NonData

NanValues = np.where(v1val == Nonvalue)[0]
cP        = np.arange(0,nelem)
cPP       = np.delete(cP, NanValues, axis=0)

XX = np.delete(DATA, NanValues, axis=0)

# Predecir los valores con el modelo entrenado

y_raster = model_classifier.predict(XX)

# Crear nuevamente el formato de datos para llevarlo al raster.
# Usaremos una variable nueva llena de zeros, en la cual insertaremos los NonValues y los valores de la predicción.
# el tamaño de esta variable será el mismo que el raster

Rasterdataarray = np.zeros((rows,col)).flatten()

# cPP contiene las posiciones donde van las predicciones
for i in range(cPP.shape[0]):
    Rasterdataarray[cPP[i]]=y_raster[i]

# NanValues contiene las posiciones donde van los NOnValues
for i in range(NanValues.shape[0]):
    Rasterdataarray[NanValues[i]]=Nonvalue


# guardar la predicción en un raster con las mismas caracteristicas del raster input

Rasterout = driver.Create('/content/drive/MyDrive/Colab Notebooks/PAPER_DL/MAPA_PROBABILIDAD/04_Pretext_Fine_Tuning.tif', col, rows, 1, gdal.GDT_Float32) #esta ruta se modifica según donde desee que se guarde el mapa de probabilidad en su Google Drive
# Write metadata
Rasterout.SetGeoTransform(raster.GetGeoTransform())
Rasterout.SetProjection(raster.GetProjection())

Rasterout.GetRasterBand(1).WriteArray(Rasterdataarray.reshape(rows,col))
Rasterout.GetRasterBand(1).SetNoDataValue(Nonvalue)
Rasterout = None
del Rasterout

# Visualizar el mapa final de probabilidad

mask = Rasterdataarray!=-9999
v2 = Rasterdataarray[np.where(mask)]
v2 = v2.reshape(-1,961)
plt.figure(figsize=(10,10))
plt.imshow(v2,cmap='jet',vmin=0, vmax=1)
plt.colorbar(fraction=0.04)

"""# **Créditos**
---

* **Autores:**
  * [Ana Gabriela Mantilla, Geóloga](https://www.linkedin.com/in/ana-gabriela-mantilla-24377a21a)
  * [Paul Goyes Peñafiel, PhD (c)](https://www.linkedin.com/in/paul-goyes-0212b810/)
"""